{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Resume Parsing with Spacy\n",
        "\n",
        "## Introduction\n",
        "In this project, we will build a resume parsing system using the Spacy library. The goal is to extract relevant information, such as entities (e.g., names, organizations, skills), from resumes in order to process and analyze them effectively.\n",
        "\n",
        "## Dataset\n",
        "We will utilize a dataset (`dataset.json`) containing resumes in JSON format. The dataset will be split into training and test sets for training and evaluating our model.\n",
        "\n",
        "## Dependencies\n",
        "Make sure you have the following dependencies installed:\n",
        "- Spacy\n",
        "- tqdm\n",
        "- scikit-learn\n",
        "- PyMuPDF\n",
        "- spacy_transformers\n",
        "## Training the Model\n",
        "1. Load the dataset and split it into training and test sets.\n",
        "2. Convert the training and test data into Spacy DocBin format.\n",
        "3. Save the converted data to disk (`train_data.spacy` and `test_data.spacy`).\n",
        "4. Train the Spacy model using the configuration file (`config.cfg`).\n",
        "5. The trained model will be saved in the `output` directory.\n",
        "\n",
        "## Parsing Resumes\n",
        "1. Load the trained model from the `output` directory.\n",
        "2. Install the PyMuPDF library for processing PDF files.\n",
        "3. Provide the path to the PDF file you want to parse.\n",
        "4. The extracted entities and their labels will be printed.\n",
        "\n",
        "## Conclusion\n",
        "Resume parsing is a useful technique for automating the extraction of information from resumes. By using Spacy and a well-prepared dataset, we can train a model to accurately identify and extract entities from resumes.\n",
        "\n",
        "Let's get started!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "import fitz"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load CV data from JSON file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hn0rRVgXd2Jn"
      },
      "outputs": [],
      "source": [
        "data = json.load(open('../data/dataset.json','r'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ry74dOnEd941",
        "outputId": "7cffb615-bad0-4eba-988b-32a6325e0e40"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1014"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x618iaFTeFlZ",
        "outputId": "47e597e6-376f-484f-db45-b4b76d708e6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✔ Auto-filled config with all values\n",
            "✔ Saved config\n",
            "..\\config\\config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy init fill-config ../config/base_config.cfg ../config/config.cfg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1nfS3Z_vfqXw"
      },
      "outputs": [],
      "source": [
        "# Function to convert data to Spacy DocBin format\n",
        "def get_spacy_doc(file, data):\n",
        "    nlp = spacy.blank(\"en\")\n",
        "    db = DocBin()\n",
        "\n",
        "    for text, annot in tqdm(data):\n",
        "        doc = nlp.make_doc(text)\n",
        "        annot = annot['entities']\n",
        "\n",
        "        ents = []\n",
        "        entity_indices = []\n",
        "\n",
        "        for start, end, label in annot:\n",
        "            skip_entity = False\n",
        "            for idx in range(start, end):\n",
        "                if idx in entity_indices:\n",
        "                    skip_entity = True\n",
        "                    break\n",
        "            if skip_entity == True:\n",
        "                continue\n",
        "\n",
        "            entity_indices = entity_indices + list(range(start, end))\n",
        "            try:\n",
        "                span = doc.char_span(\n",
        "                    start, end, label=label, alignment_mode='strict')\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            if span is None:\n",
        "                err_data = str([start, end]) + \"    \" + str(text) + \"\\n\"\n",
        "                file.write(err_data)\n",
        "            else:\n",
        "                ents.append(span)\n",
        "\n",
        "        try:\n",
        "            doc.ents = ents\n",
        "            db.add(doc)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    return db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ikeZuO7fgSHY"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into train and test sets\n",
        "train, test = train_test_split(data, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJzoyU6ShYuG",
        "outputId": "ac271fde-21ab-4108-d0c6-be7da6a46b2f"
      },
      "outputs": [],
      "source": [
        "# Open file to write training data errors\n",
        "file = open('../model/train_file.txt', 'w')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pY-vwS2Thfat"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 811/811 [00:08<00:00, 98.40it/s] \n",
            "100%|██████████| 203/203 [00:02<00:00, 92.16it/s] \n"
          ]
        }
      ],
      "source": [
        "# Convert training data to Spacy DocBin format and save to disk\n",
        "train_db = get_spacy_doc(file, train)\n",
        "train_db.to_disk('../model/train_data.spacy')\n",
        "\n",
        "# Convert test data to Spacy DocBin format and save to disk\n",
        "test_db = get_spacy_doc(file, test)\n",
        "test_db.to_disk('../model/test_data.spacy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BVxsnmuch_Fl"
      },
      "outputs": [],
      "source": [
        "file.close()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Train this model with gpu and make necessary changes in config file. recommendation is train this with google colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTAX7fVKiGMf",
        "outputId": "946c1b8f-2bda-4067-8a8e-62b1ba14f22d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-06-28 19:11:40.061569: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;2m✔ Created output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2023-06-28 19:11:47,085] [INFO] Set up nlp object from config\n",
            "[2023-06-28 19:11:47,101] [INFO] Pipeline: ['transformer', 'ner']\n",
            "[2023-06-28 19:11:47,105] [INFO] Created vocabulary\n",
            "[2023-06-28 19:11:47,105] [INFO] Finished initializing nlp object\n",
            "Downloading (…)lve/main/config.json: 100% 481/481 [00:00<00:00, 3.36MB/s]\n",
            "Downloading (…)olve/main/vocab.json: 100% 899k/899k [00:00<00:00, 14.9MB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 73.1MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 55.0MB/s]\n",
            "Downloading model.safetensors: 100% 499M/499M [00:06<00:00, 72.2MB/s]\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[2023-06-28 19:12:31,415] [INFO] Initialized pipeline components: ['transformer', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  -------------  --------  ------  ------  ------  ------\n",
            "  0       0        1213.94   2002.17    0.02    0.01    0.16    0.00\n",
            "  0     200      425524.80  90700.21   28.62   32.36   25.66    0.29\n",
            "  1     400      298785.41  39142.82   37.04   42.07   33.09    0.37\n",
            "  2     600       81650.62  19916.76   77.21   77.44   76.98    0.77\n",
            "  3     800       23235.47  12017.25   83.35   82.96   83.73    0.83\n",
            "  4    1000        9758.65  10246.89   83.71   80.93   86.70    0.84\n",
            "  5    1200        7482.83   8262.96   85.41   84.44   86.41    0.85\n",
            "  6    1400       15431.42   7512.00   85.42   84.99   85.85    0.85\n",
            "  7    1600        5178.95   6429.07   84.53   83.02   86.10    0.85\n",
            "  8    1800       13543.37   7056.29   86.45   86.23   86.68    0.86\n",
            "  9    2000        3454.27   5767.02   86.68   85.07   88.34    0.87\n",
            " 10    2200        3616.30   5443.64   86.17   83.95   88.51    0.86\n",
            " 11    2400       10447.32   5335.64   86.57   85.31   87.87    0.87\n",
            " 12    2600        6720.42   5074.00   86.53   86.10   86.96    0.87\n",
            " 13    2800        4431.54   4964.28   86.78   85.52   88.08    0.87\n",
            " 14    3000        9857.29   4783.58   85.54   83.48   87.71    0.86\n",
            " 15    3200        3008.62   4115.62   86.50   84.97   88.10    0.87\n",
            " 16    3400        7112.76   4396.92   86.64   86.08   87.21    0.87\n",
            " 17    3600        3679.58   4026.50   86.27   84.72   87.89    0.86\n",
            " 18    3800        7720.94   4054.31   86.63   85.63   87.64    0.87\n",
            " 19    4000        2109.61   3683.77   86.07   84.05   88.20    0.86\n",
            " 20    4200        6853.40   3787.01   86.63   86.54   86.72    0.87\n",
            " 21    4400       51241.15   4197.09   86.77   86.11   87.44    0.87\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "output/model-last\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy train ../config/config.cfg --output ../model/output --paths.train ../model/train_data.spacy --paths.dev ../model/test_data.spacy --gpu-id 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1nlEOGAwaxGs"
      },
      "outputs": [],
      "source": [
        "# Load the trained model\n",
        "nlp = spacy.load('../model/output/model-best')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "tIotwSG2bLXJ"
      },
      "outputs": [],
      "source": [
        "# Process the PDF file using Spacy\n",
        "import sys, fitz\n",
        "fname = '../yashnewresume.pdf'\n",
        "doc = fitz.open(fname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "LKdDCmtYbYq2"
      },
      "outputs": [],
      "source": [
        "text = \" \"\n",
        "for page in doc:\n",
        "    text = text+str(page.get_text())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-k7tNJokbeVW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Yash Parmar  ---->>> NAME\n",
            "Ahmedabad, Gujarat, 382440, india  ---->>> LOCATION\n",
            "yashp3020@gmail.com  ---->>> EMAIL ADDRESS\n",
            "K.S School of business Management  ---->>> COLLEGE NAME\n",
            "Kum Kum Vidyalaya  ---->>> NAME\n",
            "Kum Kum Vidyalaya  ---->>> NAME\n",
            "EcoSnap  ---->>> SKILLS\n",
            "SKILLS  ---->>> SKILLS\n",
            "Python  ---->>> SKILLS\n",
            "Javascript  ---->>> SKILLS\n",
            "Mongodb  ---->>> SKILLS\n",
            "MySql  ---->>> SKILLS\n",
            "ReactJS  ---->>> SKILLS\n",
            "Data Science  ---->>> SKILLS\n",
            "ML/DL  ---->>> SKILLS\n",
            "AWS  ---->>> SKILLS\n",
            "NextJS  ---->>> SKILLS\n",
            "Nestjs  ---->>> SKILLS\n",
            "Microservices  ---->>> SKILLS\n",
            "Express  ---->>> SKILLS\n",
            "NodeJS  ---->>> SKILLS\n",
            "Tensor�ow  ---->>> SKILLS\n",
            "Docker  ---->>> SKILLS\n",
            "Scikit-Learn  ---->>> SKILLS\n",
            "Gujarati  ---->>> LANGUAGE\n",
            "Hindi  ---->>> LANGUAGE\n",
            "English  ---->>> LANGUAGE\n",
            "Techathon Winner  ---->>> AWARDS\n",
            "Gateway Group  ---->>> AWARDS\n",
            "JK Laxmipat University Hackathon  ---->>> NAME\n",
            "Machine learning with python  ---->>> CERTIFICATION\n",
            "JKLU University,jaipur  ---->>> UNIVERSITY\n",
            "IBM  ---->>> COMPANIES WORKED AT\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(text)\n",
        "for ent in doc.ents:\n",
        "  print(ent.text,\" ---->>>\",ent.label_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
